\documentclass{article}
\usepackage{hyperref}
\usepackage{graphicx}

\title{pySLAM}
\author{Luigi Freda}
\date{}  % put the 

\begin{document}

\maketitle

\section{Introduction}
\textbf{pySLAM} is a Python implementation of a Visual SLAM pipeline that supports monocular, stereo, and RGBD cameras. It provides various features including local features, loop closing methods, volumetric reconstruction, depth prediction models, and more.

\section{Installation}

\subsection{Main Requirements}
\begin{itemize}
    \item Python 3.8.10
    \item OpenCV $>=$4.10
    \item PyTorch 2.3.1
    \item Tensorflow 2.13.1
    \item Kornia 0.7.3
    \item Rerun
\end{itemize}

\subsection{Ubuntu}
Follow the instructions in the \texttt{docs/PYTHON-VIRTUAL-ENVS.md} file to create a new virtual environment \texttt{pyslam} with \texttt{venv}. The procedure has been tested on Ubuntu 18.04, 20.04, 22.04, and 24.04.

\subsection{MacOS}
Follow the instructions in the \texttt{docs/MAC.md} file. The procedure was tested under Sequoia 15.1.1 and Xcode 16.1.

\subsection{Docker}
If you prefer Docker or have an unsupported OS, use \texttt{rosdocker} with its custom \texttt{pyslam} / \texttt{pyslam\_cuda} Docker files and follow the instructions \href{https://github.com/luigifreda/rosdocker#pyslam}{here}.

\subsection{How to Install Non-Free OpenCV Modules}
The provided install scripts will install a recent OpenCV version (>=4.10) with non-free modules enabled. To verify your installed OpenCV version, run:
\begin{verbatim}
$ . pyenv-activate.sh
$ ./scripts/opencv_check.py
$ python3 -c "import cv2; print(cv2.__version__)"
$ python3 -c "import cv2; detector = cv2.xfeatures2d.SURF_create()"
\end{verbatim}

\subsection{Troubleshooting and Performance Issues}
If you encounter issues or performance problems, refer to the \texttt{docs/TROUBLESHOOTING.md} file for assistance.

\section{Usage}

\subsection{Feature Tracking}
To test the basic feature tracking capabilities, run:
\begin{verbatim}
$ . pyenv-activate.sh
$ ./main_feature_matching.py
\end{verbatim}

\subsection{Loop Closing}
Different loop closing methods are available. Loop closing is enabled by default and can be disabled by setting \texttt{kUseLoopClosing=False} in \texttt{config\_parameters.py}. Configuration options can be found in \texttt{loop\_detector\_configs.py}.

\subsection{Volumetric Reconstruction Pipeline}
The volumetric reconstruction pipeline is disabled by default. Enable it by setting \texttt{kUseVolumetricIntegration=True} in \texttt{config\_parameters.py}. It works with RGBD datasets and when a depth estimator is used.

\subsection{Depth Prediction}
Depth prediction models can be utilized in both the SLAM back-end and front-end. Refer to \texttt{depth\_estimator\_factory.py} for further details. Test depth prediction by using the script \texttt{main\_depth\_prediction.py}.

\subsection{Save and Reload a Map}
When running \texttt{main\_slam.py}:
\begin{itemize}
    \item Save the current map into \texttt{data/slam\_state/map.json} by pressing the \texttt{Save} button on the GUI.
    \item Reload and visualize the saved map by running:
    \begin{verbatim}
    $ . pyenv-activate.sh
    $ ./main_map_viewer.py
    \end{verbatim}
\end{itemize}

\subsection{Relocalization in a Loaded Map}
To enable map reloading and relocalization, set \texttt{load\_state: True} in \texttt{config.yaml}.

\subsection{Trajectory Saving}
Estimated trajectories can be saved in TUM, KITTI, and EuRoC formats. Enable trajectory saving in \texttt{config.yaml}.

\subsection{SLAM GUI}
Some GUI buttons in \texttt{main\_slam.py}:
\begin{itemize}
    \item \texttt{Step}: Enter step-by-step mode.
    \item \texttt{Save}: Save the map into \texttt{map.json}.
    \item \texttt{Reset}: Reset the SLAM system.
    \item \texttt{Draw Ground Truth}: Visualize the ground truth trajectory.
\end{itemize}

\subsection{Monitor Logs}
Monitor logs for tracking, local mapping, and loop closing by running:
\begin{verbatim}
$ tail -f logs/<log file name>
$ ./scripts/launch_tmux_slam.sh
\end{verbatim}

\section{Supported Components and Models}

\subsection{Supported Local Features}
Supported feature detectors and descriptors include FAST, ORB, SIFT, SURF, SuperPoint, and more. Refer to \texttt{feature\_types.py} for more information.

\subsection{Supported Matchers}
Supported matchers include BF, FLANN, XFeat, LightGlue, and LoFTR. Refer to \texttt{feature\_matcher.py} for more details.

\subsection{Supported Global Descriptors and Local Descriptor Aggregation Methods}
Supported methods include BoW, VLAD, iBoW, HDC, SAD, AlexNet, NetVLAD, HDC-DELF, CosPlace, and EigenPlaces. Refer to \texttt{loop\_detector\_configs.py} for more details.

\subsection{Supported Depth Prediction Models}
Supported models include SGBM, Depth-Pro, DepthAnythingV2, RAFT-Stereo, and CREStereo.

\section{Datasets}
Supported datasets include KITTI, TUM, EuRoC, video files, and folders of images. Refer to \texttt{config.yaml} for configuration details.

\section{Camera Settings}
The \texttt{settings} folder contains camera settings files for testing. Use the scripts in the \texttt{calibration} folder to calibrate your camera.

\section{Comparison pySLAM vs ORB-SLAM3}
For a comparison of trajectories estimated by pySLAM and ORB-SLAM3, see the \href{https://github.com/anathonic/Trajectory-Comparison-ORB-SLAM3-pySLAM/blob/main/trajectories_comparison.ipynb}{trajectory comparison notebook}.

\section{Contributing to pySLAM}
If you like pySLAM and would like to contribute, report bugs, leave comments, and propose new features through issues and pull requests on GitHub. Contact \texttt{luigifreda@gmail.com} for more information.

\section{References}
Suggested books and materials include:
\begin{itemize}
    \item \href{https://www.robots.ox.ac.uk/~vgg/hzbook/}{Multiple View Geometry in Computer Vision} by Richard Hartley and Andrew Zisserman
    \item \href{https://link.springer.com/book/10.1007/978-0-387-21779-6}{An Invitation to 3-D Vision} by Yi-Ma, Stefano Soatto, Jana Kosecka, S. Shankar Sastry
    \item \href{http://szeliski.org/Book/}{Computer Vision: Algorithms and Applications} by Richard Szeliski
    \item \href{http://www.deeplearningbook.org/lecture_slides.html}{Deep Learning} by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
    \item \href{http://neuralnetworksanddeeplearning.com/index.html}{Neural Networks and Deep Learning} by Michael Nielsen
\end{itemize}

\section{Credits}
Credits to various libraries and contributors including Pangolin, g2opy, ORBSLAM2, SuperPointPretrainedNetwork, Tfeat, Image Matching Benchmark Baselines, Hardnet, GeoDesc, SOSNet, L2Net, Log-polar descriptor, D2-Net, DELF, Contextdesc, LFNet, R2D2, BEBLID, DISK, Xfeat, LightGlue, Key.Net, Twitchslam, MonoVO, VPR\_Tutorial, DepthAnythingV2, DepthPro, RAFT-Stereo, CREStereo, and Anathonic.

\end{document}